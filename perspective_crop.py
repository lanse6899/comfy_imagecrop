import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import math

class PerspectiveCropWithPanel:
    """
    Photoshopé£æ ¼çš„é€è§†å‰ªè£èŠ‚ç‚¹
    æ”¯æŒå››è§’ç‚¹æ‹–æ‹½å®šä¹‰é€è§†åŒºåŸŸï¼Œè‡ªåŠ¨è¿›è¡Œé€è§†æ ¡æ­£å’Œè£å‰ª
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "top_left_x": ("FLOAT", {
                    "default": 100.0,
                    "min": -4096.0,
                    "max": 4096.0,
                    "step": 1.0,
                    "display": "hidden"
                }),
                "top_left_y": ("FLOAT", {
                    "default": 100.0,
                    "min": -4096.0,
                    "max": 4096.0,
                    "step": 1.0,
                    "display": "hidden"
                }),
                "top_right_x": ("FLOAT", {
                    "default": 300.0,
                    "min": -4096.0,
                    "max": 4096.0,
                    "step": 1.0,
                    "display": "hidden"
                }),
                "top_right_y": ("FLOAT", {
                    "default": 100.0,
                    "min": -4096.0,
                    "max": 4096.0,
                    "step": 1.0,
                    "display": "hidden"
                }),
                "bottom_left_x": ("FLOAT", {
                    "default": 100.0,
                    "min": -4096.0,
                    "max": 4096.0,
                    "step": 1.0,
                    "display": "hidden"
                }),
                "bottom_left_y": ("FLOAT", {
                    "default": 300.0,
                    "min": -4096.0,
                    "max": 4096.0,
                    "step": 1.0,
                    "display": "hidden"
                }),
                "bottom_right_x": ("FLOAT", {
                    "default": 300.0,
                    "min": -4096.0,
                    "max": 4096.0,
                    "step": 1.0,
                    "display": "hidden"
                }),
                "bottom_right_y": ("FLOAT", {
                    "default": 300.0,
                    "min": -4096.0,
                    "max": 4096.0,
                    "step": 1.0,
                    "display": "hidden"
                }),
                "auto_size": ("BOOLEAN", {
                    "default": False,
                    "label_on": "è‡ªé€‚åº”",
                    "label_off": "æ‰‹åŠ¨"
                }),
                "output_width": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 2048,
                    "step": 8
                }),
                "output_height": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 2048,
                    "step": 8
                }),
                "fill_color": (["black", "white", "transparent"], {
                    "default": "black"
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "IMAGE")
    RETURN_NAMES = ("cropped_image", "preview_image")
    FUNCTION = "perspective_crop"
    CATEGORY = "ğŸ”µBB image crop"
    
    def perspective_crop(self, image, top_left_x, top_left_y, top_right_x, top_right_y,
                        bottom_left_x, bottom_left_y, bottom_right_x, bottom_right_y,
                        auto_size, output_width, output_height, fill_color):
        """
        é€è§†å‰ªè£ä¸»å‡½æ•°
        """
        # è½¬æ¢tensoråˆ°PILå›¾åƒ
        if len(image.shape) == 4:
            pil_image = self.tensor_to_pil(image[0])
        else:
            pil_image = self.tensor_to_pil(image)
        
        # ä½¿ç”¨ç”¨æˆ·åœ¨é¢æ¿ä¸­ç‚¹å‡»çš„è§’ç‚¹
        src_points = np.array([
            [top_left_x, top_left_y],           # å·¦ä¸Š
            [top_right_x, top_right_y],         # å³ä¸Š  
            [bottom_right_x, bottom_right_y],   # å³ä¸‹
            [bottom_left_x, bottom_left_y]      # å·¦ä¸‹
        ], dtype=np.float32)
        
        # å¦‚æœå¯ç”¨è‡ªé€‚åº”å°ºå¯¸ï¼Œè®¡ç®—æœ€ä½³è¾“å‡ºå°ºå¯¸
        if auto_size:
            output_width, output_height = self.calculate_adaptive_size(src_points)
        
        # å®šä¹‰ç›®æ ‡çŸ©å½¢çš„å››ä¸ªè§’ç‚¹
        dst_points = np.array([
            [0, 0],                           # å·¦ä¸Š
            [output_width, 0],                # å³ä¸Š
            [output_width, output_height],    # å³ä¸‹
            [0, output_height]                # å·¦ä¸‹
        ], dtype=np.float32)
        
        # æ‰§è¡Œé€è§†å˜æ¢
        transformed_image = self.apply_perspective_transform(
            pil_image, src_points, dst_points, output_width, output_height, fill_color
        )
        
        # ç”Ÿæˆé¢„è§ˆå›¾åƒ
        preview_image = self.create_preview(
            pil_image, src_points
        )
        
        # è½¬æ¢å›tensor
        cropped_tensor = self.pil_to_tensor(transformed_image)
        preview_tensor = self.pil_to_tensor(preview_image)
        
        return (cropped_tensor, preview_tensor)
    
    def calculate_adaptive_size(self, src_points):
        """
        æ ¹æ®é€è§†å››è¾¹å½¢è®¡ç®—æœ€ä½³è¾“å‡ºå°ºå¯¸
        """
        # è®¡ç®—å››è¾¹å½¢çš„è¾¹é•¿
        top_width = np.linalg.norm(src_points[1] - src_points[0])      # ä¸Šè¾¹
        bottom_width = np.linalg.norm(src_points[2] - src_points[3])   # ä¸‹è¾¹
        left_height = np.linalg.norm(src_points[3] - src_points[0])    # å·¦è¾¹
        right_height = np.linalg.norm(src_points[2] - src_points[1])   # å³è¾¹
        
        # å–å¹³å‡å€¼ä½œä¸ºè¾“å‡ºå°ºå¯¸
        avg_width = int((top_width + bottom_width) / 2)
        avg_height = int((left_height + right_height) / 2)
        
        # ç¡®ä¿å°ºå¯¸åœ¨åˆç†èŒƒå›´å†…
        min_size = 64
        max_size = 2048
        
        output_width = max(min_size, min(max_size, avg_width))
        output_height = max(min_size, min(max_size, avg_height))
        
        # ç¡®ä¿æ˜¯8çš„å€æ•°ï¼ˆå¸¸è§çš„å›¾åƒå¤„ç†è¦æ±‚ï¼‰
        output_width = (output_width // 8) * 8
        output_height = (output_height // 8) * 8
        
        return output_width, output_height
    
    def apply_perspective_transform(self, image, src_points, dst_points, width, height, fill_color):
        """
        åº”ç”¨é€è§†å˜æ¢
        """
        try:
            # å°è¯•ä½¿ç”¨ OpenCVï¼ˆå¦‚æœå¯ç”¨ï¼‰
            import cv2
            
            # è½¬æ¢PILå›¾åƒä¸ºnumpyæ•°ç»„
            img_array = np.array(image)
            
            # ç¡®ä¿ç‚¹çš„é¡ºåºæ­£ç¡®ï¼šå·¦ä¸Šã€å³ä¸Šã€å³ä¸‹ã€å·¦ä¸‹
            src_pts = np.float32(src_points)
            dst_pts = np.float32(dst_points)
            
            # è®¡ç®—é€è§†å˜æ¢çŸ©é˜µ
            matrix = cv2.getPerspectiveTransform(src_pts, dst_pts)
            
            # åº”ç”¨é€è§†å˜æ¢
            if fill_color == "white":
                borderValue = (255, 255, 255)
            elif fill_color == "transparent":
                borderValue = (0, 0, 0, 0)
                # ç¡®ä¿å›¾åƒæœ‰alphaé€šé“
                if len(img_array.shape) == 3 and img_array.shape[2] == 3:
                    alpha = np.ones((img_array.shape[0], img_array.shape[1], 1), dtype=img_array.dtype) * 255
                    img_array = np.concatenate([img_array, alpha], axis=2)
            else:
                borderValue = (0, 0, 0)
            
            transformed = cv2.warpPerspective(
                img_array, matrix, (width, height),
                borderMode=cv2.BORDER_CONSTANT,
                borderValue=borderValue
            )
            
            # è½¬æ¢å›PILå›¾åƒ
            if fill_color == "transparent":
                result_image = Image.fromarray(transformed, 'RGBA')
            else:
                result_image = Image.fromarray(transformed, 'RGB')
                
        except ImportError:
            # å¦‚æœæ²¡æœ‰OpenCVï¼Œä½¿ç”¨ç®€åŒ–çš„å®ç°
            result_image = self.simple_perspective_transform(
                image, src_points, dst_points, width, height, fill_color
            )
        except Exception as e:
            # å¦‚æœOpenCVå˜æ¢å¤±è´¥ï¼Œä½¿ç”¨PILå®ç°
            result_image = self.simple_perspective_transform(
                image, src_points, dst_points, width, height, fill_color
            )
        
        return result_image
    
    def simple_perspective_transform(self, image, src_points, dst_points, width, height, fill_color):
        """
        ç®€åŒ–çš„é€è§†å˜æ¢å®ç°ï¼ˆå½“OpenCVä¸å¯ç”¨æ—¶ï¼‰
        ä½¿ç”¨PILçš„transformæ–¹æ³•
        """
        # åˆ›å»ºè¾“å‡ºå›¾åƒ
        if fill_color == "white":
            result_image = Image.new('RGB', (width, height), (255, 255, 255))
        elif fill_color == "transparent":
            result_image = Image.new('RGBA', (width, height), (0, 0, 0, 0))
            if image.mode != 'RGBA':
                image = image.convert('RGBA')
        else:
            result_image = Image.new('RGB', (width, height), (0, 0, 0))
        
        try:
            # ä½¿ç”¨PILçš„transformæ–¹æ³•è¿›è¡Œé€è§†å˜æ¢
            coeffs = self.find_coeffs(dst_points, src_points)
            
            # åº”ç”¨é€è§†å˜æ¢
            transformed = image.transform(
                (width, height),
                Image.Transform.PERSPECTIVE,
                coeffs,
                Image.Resampling.BICUBIC
            )
            
            return transformed
            
        except Exception as e:
            # å¦‚æœå˜æ¢å¤±è´¥ï¼Œè¿”å›è°ƒæ•´å¤§å°çš„åŸå›¾
            return image.resize((width, height), Image.Resampling.BICUBIC)
    
    def find_coeffs(self, pa, pb):
        """
        è®¡ç®—PILé€è§†å˜æ¢ç³»æ•°
        """
        matrix = []
        for p1, p2 in zip(pa, pb):
            matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])
            matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])

        A = np.matrix(matrix, dtype=np.float64)
        B = np.array(pb).reshape(8)

        try:
            res = np.dot(np.linalg.inv(A.T * A) * A.T, B)
            return np.array(res).reshape(8)
        except:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›å•ä½å˜æ¢
            return [1, 0, 0, 0, 1, 0, 0, 0]
    
    def create_preview(self, image, src_points):
        """
        åˆ›å»ºé¢„è§ˆå›¾åƒï¼Œæ˜¾ç¤ºé€è§†å››è¾¹å½¢
        """
        preview = image.copy().convert('RGB')
        draw = ImageDraw.Draw(preview)
        
        # ç»˜åˆ¶é€è§†å››è¾¹å½¢
        points = [(int(p[0]), int(p[1])) for p in src_points]
        
        # ç»˜åˆ¶å››è¾¹å½¢è¾¹æ¡†
        for i in range(4):
            start = points[i]
            end = points[(i + 1) % 4]
            draw.line([start, end], fill=(0, 255, 255), width=3)
        
        # ç»˜åˆ¶è§’ç‚¹
        for i, point in enumerate(points):
            x, y = point
            size = 8
            
            # ä¸åŒé¢œè‰²æ ‡è¯†ä¸åŒè§’ç‚¹
            colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]
            color = colors[i]
            
            draw.ellipse([x-size, y-size, x+size, y+size], 
                        fill=color, outline=(255, 255, 255), width=2)
        
        # æ˜¾ç¤ºè§’ç‚¹æ ‡ç­¾
        try:
            font = ImageFont.truetype("arial.ttf", 16)
        except:
            font = None
        
        labels = ["å·¦ä¸Š", "å³ä¸Š", "å³ä¸‹", "å·¦ä¸‹"]
        for i, (point, label) in enumerate(zip(points, labels)):
            x, y = point
            draw.text((x + 12, y - 8), label, fill=(255, 255, 255), font=font)
        
        return preview
    
    def tensor_to_pil(self, tensor):
        """å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        if len(tensor.shape) == 3:
            if tensor.shape[0] == 3 or tensor.shape[0] == 1:
                tensor = tensor.permute(1, 2, 0)
        
        tensor = torch.clamp(tensor, 0, 1)
        numpy_image = tensor.cpu().numpy()
        numpy_image = (numpy_image * 255).astype(np.uint8)
        
        if len(numpy_image.shape) == 2:
            pil_image = Image.fromarray(numpy_image, mode='L').convert('RGB')
        elif numpy_image.shape[2] == 1:
            pil_image = Image.fromarray(numpy_image.squeeze(), mode='L').convert('RGB')
        else:
            pil_image = Image.fromarray(numpy_image, mode='RGB')
        
        return pil_image
    
    def pil_to_tensor(self, pil_image):
        """å°†PILå›¾åƒè½¬æ¢ä¸ºtensor"""
        if pil_image.mode == 'RGBA':
            pass
        elif pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        numpy_image = np.array(pil_image).astype(np.float32)
        numpy_image = numpy_image / 255.0
        tensor = torch.from_numpy(numpy_image)
        tensor = tensor.unsqueeze(0)
        
        return tensor


NODE_CLASS_MAPPINGS = {
    "PerspectiveCropWithPanel": PerspectiveCropWithPanel,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "PerspectiveCropWithPanel": "ğŸ”µBBé€è§†å‰ªè£",
}
